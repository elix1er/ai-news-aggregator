---
title: "Have we hit a scaling wall in base models? (non reasoning)"
date: "2025-02-21T12:28:05.000Z"
link: "https://www.reddit.com/r/artificial/comments/1iupqgp/have_we_hit_a_scaling_wall_in_base_models_non/"
---

Grok 3 was supposedly trained on 100,000 H100 GPUs, which is in the ballpark of about 10x more than models like the GPT-4 series and Claude 3.5 Sonnet
 Yet they're about equal in abilities. Grok 3 isn...

[Read more](https://www.reddit.com/r/artificial/comments/1iupqgp/have_we_hit_a_scaling_wall_in_base_models_non/)
