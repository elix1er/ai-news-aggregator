---
title: "New study: More alignment training might be backfiring in LLM safety (DeepTeam red teaming results)"
date: "2025-06-18T05:13:21.000Z"
link: "https://www.reddit.com/r/artificial/comments/1le8vw7/new_study_more_alignment_training_might_be/"
---

TL;DR: Heavily-aligned models (DeepSeek-R1, o3, o4-mini) had 24.1% breach rate vs 21.0% for lightly-aligned models (GPT-3.5/4, Claude 3.5 Haiku) when facing sophisticated attacks. More safety training...

[Read more](https://www.reddit.com/r/artificial/comments/1le8vw7/new_study_more_alignment_training_might_be/)
