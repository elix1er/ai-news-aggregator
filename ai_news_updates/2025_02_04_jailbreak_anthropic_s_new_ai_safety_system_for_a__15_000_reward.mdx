---
title: "Jailbreak Anthropic's new AI safety system for a $15,000 reward"
date: "2025-02-04T19:35:00.000Z"
link: "https://www.zdnet.com/article/jailbreak-anthropics-new-ai-safety-system-for-a-15000-reward/"
---

In testing, the technique helped Claude block 95% of jailbreak attempts. But the process still needs more 'real-world' red-teaming.

[Read more](https://www.zdnet.com/article/jailbreak-anthropics-new-ai-safety-system-for-a-15000-reward/)
