---
title: "Anthropic offers $20,000 to whoever can jailbreak its new AI safety system"
date: "2025-02-06T15:54:00.000Z"
link: "https://www.zdnet.com/article/anthropic-offers-20000-to-whoever-can-jailbreak-its-new-ai-safety-system/"
---

The company has upped its reward for red-teaming Constitutional Classifiers. Here's how to try.

[Read more](https://www.zdnet.com/article/anthropic-offers-20000-to-whoever-can-jailbreak-its-new-ai-safety-system/)
