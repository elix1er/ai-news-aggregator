---
title: "Researchers warn of ‘catastrophic overtraining’ in Large Language Models"
date: "2025-03-28T20:01:20.000Z"
link: "https://venturebeat.com/ai/researchers-warn-of-catastrophic-overtraining-in-large-language-models/"
---

The researchers compared two versions of OLMo-1b: one pre-trained on 2.3 trillion tokens and another on 3 trillion tokens.

[Read more](https://venturebeat.com/ai/researchers-warn-of-catastrophic-overtraining-in-large-language-models/)
